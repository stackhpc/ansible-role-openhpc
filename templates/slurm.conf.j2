#
# Example slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
#
# slurm.conf file generated by configurator.html.
#
# See the slurm.conf man page for more information.
#
ClusterName={{ openhpc_cluster_name }}
SlurmctldHost={{ openhpc_slurm_control_host }}{% if openhpc_slurm_control_host_address is defined %}({{ openhpc_slurm_control_host_address }}){% endif %}

#DisableRootJobs=NO
#EnforcePartLimits=NO
#EpilogSlurmctld=
#FirstJobId=1
#MaxJobId=67043328
#GresTypes=
#GroupUpdateForce=0
#GroupUpdateTime=600
#JobFileAppend=0
#JobRequeue=1
#JobSubmitPlugins=lua
#KillOnBadExit=0
#LaunchType=launch/slurm
#Licenses=foo*4,bar
#MailProg=/bin/mail
#MaxJobCount=10000
#MaxStepCount=40000
#MaxTasksPerNode=512
MpiDefault=none
#MpiParams=ports=#-#
#PluginDir=
#PlugStackConfig=
#PrivateData=jobs
ProctrackType=proctrack/linuxproc # TODO: really want cgroup but needs cgroup.conf and workaround for CI
#Prolog=
#PrologFlags=
#PrologSlurmctld=
#PropagatePrioProcess=0
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
#RebootProgram=
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurm # NB: not OpenHPC default!
SlurmUser=slurm
#SlurmdUser=root
#SrunEpilog=
#SrunProlog=
StateSaveLocation={{ openhpc_state_save_location }}
SwitchType=switch/none
#TaskEpilog=
#TaskPlugin=task/affinity
#TaskProlog=
#TopologyPlugin=topology/tree
#TmpFS=/tmp
#TrackWCKey=no
#TreeWidth=
#UnkillableStepProgram=
#UsePAM=0
#
#
# TIMERS
#BatchStartTimeout=10
#CompleteWait=0
#EpilogMsgTime=2000
#GetEnvTimeout=2
#HealthCheckInterval=0
#HealthCheckProgram=
InactiveLimit=0
KillWait=30
#MessageTimeout=10
#ResvOverRun=0
MinJobAge=300
#OverTimeLimit=0
SlurmctldTimeout=300
SlurmdTimeout=300
#UnkillableStepTimeout=60
#VSizeFactor=0
Waittime=0
#
#
# SCHEDULING
#DefMemPerCPU=0
#MaxMemPerCPU=0
#SchedulerTimeSlice=30
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core
#
#
# JOB PRIORITY
#PriorityFlags=
PriorityType=priority/multifactor
#PriorityDecayHalfLife=
#PriorityCalcPeriod=
#PriorityFavorSmall=
#PriorityMaxAge=
#PriorityUsageResetPeriod=
#PriorityWeightAge=
#PriorityWeightFairshare=
#PriorityWeightJobSize=
PriorityWeightPartition=1000
#PriorityWeightQOS=
PreemptType=preempt/partition_prio
PreemptMode=SUSPEND,GANG
#
# LOGGING AND ACCOUNTING
#AccountingStorageEnforce=0
AccountingStorageHost={{ openhpc_slurm_accounting_storage_host }}
{% if openhpc_slurm_accounting_storage_pass | default(false, true) %}
AccountingStoragePass={{ openhpc_slurm_accounting_storage_pass }}
{% endif %}
AccountingStoragePort={{ openhpc_slurm_accounting_storage_port }}
AccountingStorageType={{ openhpc_slurm_accounting_storage_type }}
AccountingStorageUser={{ openhpc_slurm_accounting_storage_user }}
#AccountingStoreFlags=
#JobCompHost=
JobCompLoc={{ openhpc_slurm_job_comp_loc }}
#JobCompPass=
#JobCompPort=
JobCompType={{ openhpc_slurm_job_comp_type }}
#JobCompUser=
#JobContainerType=job_container/none
JobAcctGatherFrequency={{ openhpc_slurm_job_acct_gather_frequency }}
JobAcctGatherType={{ openhpc_slurm_job_acct_gather_type }}

# By default, SLURM will log to syslog, which is what we want
SlurmctldSyslogDebug=info
SlurmdSyslogDebug=info
#SlurmSchedLogFile=
#SlurmSchedLogLevel=
#DebugFlags=

# LOGIN-ONLY NODES
# Define slurmd nodes not in partitions for login-only nodes in "configless" mode:
{%if openhpc_login_only_nodes %}{% for node in groups[openhpc_login_only_nodes] %}
NodeName={{ node }}
{% endfor %}{% endif %}

PropagateResourceLimitsExcept=MEMLOCK
Epilog=/etc/slurm/slurm.epilog.clean

# COMPUTE NODES
{% for nodegroup in openhpc_nodegroups %}
# nodegroup: {{ nodegroup.name }}
{%  set inventory_group_name = openhpc_cluster_name ~ '_' ~ nodegroup.name %}
{%  set inventory_group_hosts = groups.get(inventory_group_name, []) %}
{%      if inventory_group_hosts | length > 0 %}
{%          set play_group_hosts = inventory_group_hosts | intersect (play_hosts) %}
{%          set first_host = play_group_hosts | first | mandatory('Inventory group "' ~ inventory_group_name ~ '" contains no hosts in this play - was --limit used?') %}
{%          set first_host_hv = hostvars[first_host] %}
{%          set ram_mb = (first_host_hv['ansible_memory_mb']['real']['total'] * (nodegroup.ram_multiplier | default(openhpc_ram_multiplier))) | int %}
{%          set hostlists = (inventory_group_hosts | hostlist_expression) %}{# hosts in inventory group aren't necessarily a single hostlist expression #}
NodeName={{ hostlists | join(',') }} {{ '' -}}
    Features={{ (['nodegroup_' ~ nodegroup.name] + nodegroup.features | default([]) ) | join(',') }} {{ '' -}}
    State=UNKNOWN {{ '' -}}
    RealMemory={{ nodegroup.ram_mb | default(ram_mb) }} {{ '' -}}
    Sockets={{ first_host_hv['ansible_processor_count'] }} {{ '' -}}
    CoresPerSocket={{ first_host_hv['ansible_processor_cores'] }} {{ '' -}}
    ThreadsPerCore={{ first_host_hv['ansible_processor_threads_per_core'] }} {{ '' -}}
    {{ nodegroup.node_params | default({}) | dict2parameters }} {{ '' -}}
    {% if 'gres' in nodegroup %}Gres={{ ','.join(nodegroup.gres | map(attribute='conf')) }}{% endif %}

{%      endif %}{# 1 or more hosts in inventory #}
NodeSet=nodegroup_{{ nodegroup.name }} Feature=nodegroup_{{ nodegroup.name }}

{% endfor %}

# Define a non-existent node, in no partition, so that slurmctld starts even with all partitions empty
NodeName=nonesuch

# PARTITIONS
{% for partition in openhpc_partitions %}
PartitionName={{partition.name}} {{ '' -}}
    Default={{ partition.get('default', 'YES') }} {{ '' -}}
    MaxTime={{ partition.get('maxtime', openhpc_job_maxtime) }} {{ '' -}}
    State=UP  {{ '' -}}
    Nodes={{ partition.get('nodegroups', [partition.name]) | map('regex_replace', '^', 'nodegroup_') | join(',') }} {{ '' -}}
    {{ partition.partition_params | default({}) | dict2parameters }}
{% endfor %}{# openhpc_partitions #}

{% if openhpc_slurm_configless | bool %}SlurmctldParameters=enable_configless{% endif %}


ReturnToService=2
