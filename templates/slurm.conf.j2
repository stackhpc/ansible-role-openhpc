#
# Example slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
#
# slurm.conf file generated by configurator.html.
#
# See the slurm.conf man page for more information.
#
ClusterName={{ openhpc_cluster_name }}
ControlMachine={{ openhpc_slurm_control_host }}
#ControlAddr=
#BackupController=
#BackupAddr=
#
SlurmUser=slurm
#SlurmdUser=root
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
StateSaveLocation={{ openhpc_state_save_location }}
SlurmdSpoolDir=/var/spool/slurm
SwitchType=switch/none
MpiDefault=none
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/pgid
#PluginDir=
#FirstJobId=
#MaxJobCount=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
#Prolog=
#Epilog=
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=
#TaskPlugin=
#TrackWCKey=no
#TreeWidth=50
#TmpFS=
#UsePAM=
{% if openhpc_slurm_configless %}SlurmctldParameters=enable_configless{% endif %}


# TIMERS
SlurmctldTimeout=300
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0
#
# SCHEDULING
SchedulerType=sched/backfill
#SchedulerAuth=
#SchedulerPort=
#SchedulerRootFilter=
SelectType=select/cons_res
SelectTypeParameters=CR_Core
#FastSchedule=0
PriorityType=priority/multifactor
#PriorityDecayHalfLife=14-0
#PriorityUsageResetPeriod=14-0
#PriorityWeightFairshare=100000
#PriorityWeightAge=1000
PriorityWeightPartition=1000
#PriorityWeightJobSize=1000
#PriorityMaxAge=1-0
PreemptType=preempt/partition_prio
PreemptMode=SUSPEND,GANG
#
# LOGGING
# By default, SLURM will log to syslog, which is what we want
SlurmctldSyslogDebug=info
SlurmdSyslogDebug=info
#
# ACCOUNTING
JobAcctGatherType={{ openhpc_slurm_job_acct_gather_type }}
JobAcctGatherFrequency={{ openhpc_slurm_job_acct_gather_frequency }}
JobCompType={{ openhpc_slurm_job_comp_type }}
JobCompLoc={{ openhpc_slurm_job_comp_loc }}
#
AccountingStorageType={{ openhpc_slurm_accounting_storage_type }}
AccountingStorageHost={{ openhpc_slurm_accounting_storage_host }}
AccountingStoragePort={{ openhpc_slurm_accounting_storage_port }}
{% if openhpc_slurm_accounting_storage_pass | default(false, true) %}
AccountingStoragePass={{ openhpc_slurm_accounting_storage_pass }}
{% endif %}
AccountingStorageUser={{ openhpc_slurm_accounting_storage_user }}

# LOGIN-ONLY NODES
# Define slurmd nodes not in partitions for login-only nodes in "configless" mode:
{%if openhpc_login_only_nodes %}{% for node in groups[openhpc_login_only_nodes] %}
NodeName={{ node }}
{% endfor %}{% endif %}

# COMPUTE NODES
# OpenHPC default configuration
PropagateResourceLimitsExcept=MEMLOCK
Epilog=/etc/slurm/slurm.epilog.clean
{% for part in openhpc_slurm_partitions %}
    {% set nodelist = [] %}
    {% for group in part.get('groups', [part]) %}
        {% set group_name = group.cluster_name|default(openhpc_cluster_name) ~ '_' ~ group.name %}
# openhpc_slurm_partitions: {{ group_name }}
        {% set inventory_group_hosts = groups.get(group_name, []) %}
        {% if inventory_group_hosts | length > 0 or 'cloud_nodes' in group %}{# need to template *some* nodes #}
            {% set play_group_hosts = inventory_group_hosts | intersect (play_hosts) %}
            {% set first_host_hv = hostvars[ play_group_hosts | first ] if play_group_hosts | length > 0 else {} %}
            {% set group_error_msg = 'Either inventory group "' ~ group_name ~ '" must have a host with facts in the current play or `openhpc_slurm_partitions` group "' ~ group.name ~ '" must specify ' %}
            {% if 'ram_mb' in group %}
                {% set ram_mb = group.ram_mb %}
            {% elif first_host_hv %}
                {% set ram_mb = (first_host_hv['ansible_memory_mb']['real']['total'] * (group.ram_multiplier | default(openhpc_ram_multiplier))) | int %}
            {% else %}
                {% set ram_mb = 'ERROR' %}
            {% endif %}
            {% set sockets = group.get('sockets', first_host_hv.get('ansible_processor_count', 'ERROR')) %}
            {% set cores_per_socket = group.get('cores_per_socket', first_host_hv.get('ansible_processor_cores', 'ERROR')) %}
            {% set threads_per_core = group.get('threads_per_core', first_host_hv.get('ansible_processor_threads_per_core', 'ERROR')) %}
            {% if 'error' in [ram_mb, sockets, cores_per_socket, threads_per_core] %}
                {{ false | mandatory(group_error_msg ~ 'TODO') }}{# TODO: FIXME: #}
            {% endif %}
            {% if inventory_group_hosts | length > 0 %}{# physical nodes #}
NodeName=DEFAULT State=UNKNOWN RealMemory={{ ram_mb }} Sockets={{ sockets }} CoresPerSocket={{ cores_per_socket }} ThreadsPerCore={{ threads_per_core }}
                {% for hostlist in (group_name | hostlist_expression) %}
NodeName={{ hostlist }}
                    {% set _ = nodelist.append(hostlist) %}
                {% endfor %}{# nodes #}
            {% endif %}{# inventory_group_hosts #}
            {% if 'cloud_nodes' in group %}
                {% set cloud_hostlist = (group_name | replace('_', '-')) ~ '-cloud-[0-' ~ (group['cloud_nodes'] - 1) ~ ']' %}
                {% set _ = nodelist.append(cloud_hostlist) %}
NodeName=DEFAULT State=CLOUD RealMemory={{ ram_mb }} Sockets={{ sockets }} CoresPerSocket={{ cores_per_socket }} ThreadsPerCore={{ threads_per_core }}
NodeName={{ cloud_hostlist }} State=CLOUD
            {% endif %}
        {% endif %}{# non-empty group #}
    {% endfor %}{# group #}
PartitionName={{part.name}} Default={{ {true:'YES', false:'NO'}[part.get('default', true)] }} MaxTime={{ part.get('maxtime', openhpc_job_maxtime) }} State=UP Nodes={{ nodelist | join(',') }}
{% endfor %}{# partitions #}

# Want nodes that drop out of SLURM's configuration to be automatically
# returned to service when they come back.
ReturnToService=2
