#
# Example slurm.conf file. Please run configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
#
# slurm.conf file generated by configurator.html.
#
# See the slurm.conf man page for more information.
#
ClusterName={{ openhpc_cluster_name }}
SlurmctldHost={{ openhpc_slurm_control_host }}{% if openhpc_slurm_control_host_address is defined %}({{ openhpc_slurm_control_host_address }}){% endif %}
#SlurmctldHost=
#
#DisableRootJobs=NO
#EnforcePartLimits=NO
#EpilogSlurmctld=
#FirstJobId=1
#MaxJobId=67043328
#GresTypes=
#GroupUpdateForce=0
#GroupUpdateTime=600
#JobFileAppend=0
#JobRequeue=1
#JobSubmitPlugins=lua
#KillOnBadExit=0
#LaunchType=launch/slurm
#Licenses=foo*4,bar
#MailProg=/bin/mail
#MaxJobCount=10000
#MaxStepCount=40000
#MaxTasksPerNode=512
MpiDefault=none
#MpiParams=ports=#-#
#PluginDir=
#PlugStackConfig=
#PrivateData=jobs
ProctrackType=proctrack/linuxproc # TODO: really want cgroup but needs cgroup.conf and workaround for CI
#Prolog=
#PrologFlags=
#PrologSlurmctld=
#PropagatePrioProcess=0
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
#RebootProgram=
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmctldPort=6817
SlurmdPidFile=/var/run/slurmd.pid
SlurmdPort=6818
SlurmdSpoolDir=/var/spool/slurm # NB: not OpenHPC default!
SlurmUser=slurm
#SlurmdUser=root
#SrunEpilog=
#SrunProlog=
StateSaveLocation={{ openhpc_state_save_location }}
SwitchType=switch/none
#TaskEpilog=
#TaskPlugin=task/affinity
#TaskProlog=
#TopologyPlugin=topology/tree
#TmpFS=/tmp
#TrackWCKey=no
#TreeWidth=
#UnkillableStepProgram=
#UsePAM=0
#
#
# TIMERS
#BatchStartTimeout=10
#CompleteWait=0
#EpilogMsgTime=2000
#GetEnvTimeout=2
#HealthCheckInterval=0
#HealthCheckProgram=
InactiveLimit=0
KillWait=30
#MessageTimeout=10
#ResvOverRun=0
MinJobAge=300
#OverTimeLimit=0
SlurmctldTimeout=300
SlurmdTimeout=300
#UnkillableStepTimeout=60
#VSizeFactor=0
Waittime=0
#
#
# SCHEDULING
#DefMemPerCPU=0
#MaxMemPerCPU=0
#SchedulerTimeSlice=30
SchedulerType=sched/backfill
SelectType=select/cons_tres
SelectTypeParameters=CR_Core
#
#
# JOB PRIORITY
#PriorityFlags=
PriorityType=priority/multifactor
#PriorityDecayHalfLife=
#PriorityCalcPeriod=
#PriorityFavorSmall=
#PriorityMaxAge=
#PriorityUsageResetPeriod=
#PriorityWeightAge=
#PriorityWeightFairshare=
#PriorityWeightJobSize=
PriorityWeightPartition=1000
#PriorityWeightQOS=
PreemptType=preempt/partition_prio
PreemptMode=SUSPEND,GANG
#
# LOGGING AND ACCOUNTING
#AccountingStorageEnforce=0
AccountingStorageHost={{ openhpc_slurm_accounting_storage_host }}
{% if openhpc_slurm_accounting_storage_pass | default(false, true) %}
AccountingStoragePass={{ openhpc_slurm_accounting_storage_pass }}
{% endif %}
AccountingStoragePort={{ openhpc_slurm_accounting_storage_port }}
AccountingStorageType={{ openhpc_slurm_accounting_storage_type }}
AccountingStorageUser={{ openhpc_slurm_accounting_storage_user }}
#AccountingStoreFlags=
#JobCompHost=
JobCompLoc={{ openhpc_slurm_job_comp_loc }}
#JobCompPass=
#JobCompPort=
JobCompType={{ openhpc_slurm_job_comp_type }}
#JobCompUser=
#JobContainerType=job_container/none
JobAcctGatherFrequency={{ openhpc_slurm_job_acct_gather_frequency }}
JobAcctGatherType={{ openhpc_slurm_job_acct_gather_type }}

# By default, SLURM will log to syslog, which is what we want
SlurmctldSyslogDebug=info
SlurmdSyslogDebug=info
#SlurmSchedLogFile=
#SlurmSchedLogLevel=
#DebugFlags=
#
#
# POWER SAVE SUPPORT FOR IDLE NODES - NOT SUPPORTED IN THIS APPLIANCE VERSION

# LOGIN-ONLY NODES
# Define slurmd nodes not in partitions for login-only nodes in "configless" mode:
{%if openhpc_login_only_nodes %}{% for node in groups[openhpc_login_only_nodes] %}
NodeName={{ node }}
{% endfor %}{% endif %}

# COMPUTE NODES
# OpenHPC default configuration
PropagateResourceLimitsExcept=MEMLOCK
Epilog=/etc/slurm/slurm.epilog.clean
{% for part in openhpc_slurm_partitions %}
    {% set nodelist = [] %}
    {% for group in part.get('groups', [part]) %}
        
        {% set group_name = group.cluster_name|default(openhpc_cluster_name) ~ '_' ~ group.name %}
# openhpc_slurm_partitions group: {{ group_name }}
        {% set inventory_group_hosts = groups.get(group_name, []) %}
        {% if inventory_group_hosts | length > 0 %}
            {% set play_group_hosts = inventory_group_hosts | intersect (play_hosts) %}
            {% set first_host = play_group_hosts | first | mandatory('Group "' ~ group_name ~ '" contains no hosts in this play - was --limit used?') %}
            {% set first_host_hv = hostvars[first_host] %}
            {% set ram_mb = (first_host_hv['ansible_memory_mb']['real']['total'] * (group.ram_multiplier | default(openhpc_ram_multiplier))) | int %}
            {% for hostlist in (inventory_group_hosts | hostlist_expression) %}
        {% set gres = ' Gres=%s' % (','.join(group.gres | map(attribute='conf') )) if 'gres' in group else '' %}

NodeName={{ hostlist }} State=UNKNOWN RealMemory={{ group.get('ram_mb', ram_mb) }} Sockets={{first_host_hv['ansible_processor_count']}} CoresPerSocket={{ first_host_hv['ansible_processor_cores'] }} ThreadsPerCore={{ first_host_hv['ansible_processor_threads_per_core'] }}{{ gres }}
                {% set _ = nodelist.append(hostlist) %}
            {% endfor %}{# nodes #}
        {% endif %}{# inventory_group_hosts #}
        {% for extra_node_defn in group.get('extra_nodes', []) %}
{{ extra_node_defn.items() | map('join', '=') | join(' ') }}
            {% set _ = nodelist.append(extra_node_defn['NodeName']) %}
        {% endfor %}
    {% endfor %}{# group #}
{% if not nodelist %}{# empty partition #}
{%  set nodelist = ['""'] %}
{% endif %}
PartitionName={{part.name}} Default={{ part.get('default', 'YES') }} MaxTime={{ part.get('maxtime', openhpc_job_maxtime) }} State=UP Nodes={{ nodelist | join(',') }} {{ part.partition_params | default({}) | dict2parameters }}
{% endfor %}{# partitions #}

# Define a non-existent node, in no partition, so that slurmctld starts even with all partitions empty
NodeName=nonesuch

{% if openhpc_slurm_configless %}SlurmctldParameters=enable_configless{% endif %}
ReturnToService=2
