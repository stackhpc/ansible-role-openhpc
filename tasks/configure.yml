---
# all tasks here are taken from runtime.yml - see bottom of file for what has been moved elsewhere
- name: Fail if openhpc_slurm_control_host or openhpc_cluster_name or openhpc_slurm_partitions are undefined
  fail:
    msg: "Undefined openhpc_slurm_control_host or openhpc_cluster_name or openhpc_slurm_partitions."
  when:
    openhpc_slurm_control_host == none or
    openhpc_cluster_name == none or
    openhpc_slurm_partitions | length == 0

- name: Ensure the Slurm spool directory exists
  file:
    path: /var/spool/slurm
    owner: slurm
    group: slurm
    mode: 0755
    state: directory

- name: Ensure the Munge service is enabled
  service:
    name: munge
    enabled: "{{ openhpc_slurm_service_enabled | bool }}"
  notify:
    - Restart Munge service

- name: Generate a Munge key for the platform
  command: "dd if=/dev/urandom of=/etc/munge/munge.key bs=1 count=1024"
  args:
    creates: "/etc/munge/munge.key"
  when: inventory_hostname == openhpc_slurm_control_host

- name: Retrieve Munge key from Slurm control host
  slurp:
    src: "/etc/munge/munge.key"
  register: slurm_munge_key
  when: inventory_hostname == openhpc_slurm_control_host

- name: Write Munge key
  copy:
    content: "{{ hostvars[openhpc_slurm_control_host]['slurm_munge_key']['content'] | b64decode }}"
    dest: "/etc/munge/munge.key"
    owner: munge
    group: munge
    mode: 0400
  notify:
    - Restart Munge service

# NB: Think this is correct, but should disable before doing things which cause a reboot.
- name: Ensure Slurm services are enabled
  service:
    name: "{{ openhpc_slurm_service }}"
    enabled: "{{ openhpc_slurm_service_enabled | bool }}"

- name: Ensure slurm config directory exists & world-readable
  file:
    path: "{{ openhpc_slurm_conf.location | dirname }}"
    state: directory
    owner: root
    group: root
    mode: 0755

- name: Apply customised SLURM configuration
  template:
    src: slurm.conf.j2
    dest: "{{ openhpc_slurm_conf.location }}"
    owner: root
    group: root
    mode: 0644
  run_once: "{{ openhpc_slurm_conf.shared_fs }}"

- name: Set slurm.conf location for slurm daemons
  lineinfile:
    path: "/etc/sysconfig/{{ openhpc_slurm_service }}"
    line: '{{ openhpc_slurm_service | upper }}_OPTIONS="-f {{ openhpc_slurm_conf.location }}"'
    regexp: '^{{ openhpc_slurm_service | upper }}\w+D_OPTIONS'
    create: yes
  when: openhpc_slurm_conf.location != '/etc/slurm.conf'
  register: set_slurm_conf_location

- name: Wait for network filesystem mount before daemon start
  block:
    - name: Generate systemd mount unit name for slurm.conf mountpoint
      shell:
        cmd: "systemd-escape -p --suffix=mount $(df -P {{ openhpc_slurm_conf.location }} | tail -1 | awk '{ print $6 }')"
      register: slurmconf_unit
    - name: check if mount unit exists # won't for exporting node
      shell:
        cmd: "systemctl list-units --type=mount {{ slurmconf_unit.stdout }}"
      register: slurmconf_unit_exists
      ignore_errors: true
    - name: create drop-in for systemd unit
      lineinfile:
        path: "/etc/systemd/system/{{ openhpc_slurm_service }}.service.d/slurmconf.conf"
        line: "After={{ slurmconf_unit.stdout }}"
        regexp: "After={{ slurmconf_unit.stdout }}"
        create: yes
      when: slurmconf_unit_exists is succeeded
    - name: reload systemd configuration
      systemd:
        daemon_reload: yes
  when: openhpc_slurm_conf.shared_fs # NB won't be network fs on the exporting node!

# tasks in runtime.yml which aren't here:
# - name: Ensure selected OpenHPC packages are installed => done by install action now
# - name: Install OpenHPC LMOD => done by install (separate task not in package list - why??)
# - name: Flush handlers => no longer reqd
# - name: Ensure Munge services are enabled and started => now done by start action
# - name: Ensure Slurm services are running => now done by start action
