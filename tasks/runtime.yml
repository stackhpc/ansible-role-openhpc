---

- include_tasks: pre.yml

- name: Fail if control host not in play and munge key not specified
  fail:
    msg: "Either the slurm control node must be in the play or `openhpc_munge_key` must be set"
  when:
    - openhpc_slurm_control_host not in ansible_play_hosts
    - not openhpc_munge_key

- name: Ensure Slurm directories exists
  file:
    path: "{{ item.path }}"
    owner: slurm
    group: slurm
    mode: '0755'
    state: directory
  loop:
    - path: "{{ openhpc_state_save_location }}" # StateSaveLocation
      enable: control
    - path: "{{ openhpc_slurm_conf_path | dirname }}"
      enable: control
    - path: "{{ openhpc_slurmd_spool_dir }}" # SlurmdSpoolDir
      enable: batch
  when: "openhpc_enable[item.enable] | default(false) | bool"

- name: Retrieve Munge key from control host
  # package install generates a node-unique one
  slurp:
    src: "/etc/munge/munge.key"
  register: openhpc_control_munge_key
  delegate_to: "{{ openhpc_slurm_control_host }}"
  when: openhpc_slurm_control_host in ansible_play_hosts

- name: Write Munge key
  copy:
    content: "{{ (openhpc_munge_key_b64 or openhpc_control_munge_key.content) | b64decode }}"
    dest: "/etc/munge/munge.key"
    owner: munge
    group: munge
    mode: '0400'
  register: _openhpc_munge_key_copy

- name: Ensure JobComp logfile exists
  file:
    path: "{{ openhpc_slurm_job_comp_loc }}"
    state: touch
    owner: slurm
    group: slurm
    mode: '0644'
    access_time: preserve
    modification_time: preserve
  when: openhpc_slurm_job_comp_type == 'jobcomp/filetxt'

- name: Template slurmdbd.conf
  template:
    src: slurmdbd.conf.j2
    dest: "{{ openhpc_slurm_conf_path | dirname }}/slurmdbd.conf"
    mode: "0600"
    owner: slurm
    group: slurm
  notify: Restart slurmdbd service
  when: openhpc_enable.database | default(false) | bool

- name: Template slurm.conf
  template:
    src: "{{ openhpc_slurm_conf_template }}"
    dest: "{{ openhpc_slurm_conf_path }}"
    owner: root
    group: root
    mode: '0644'
  when: openhpc_enable.control | default(false)
  notify:
    - Restart slurmctld service
  register: ohpc_slurm_conf
  # NB uses restart rather than reload as number of nodes might have changed

- name: Create gres.conf
  template:
    src: "{{ openhpc_gres_template }}"
    dest: "{{ openhpc_slurm_conf_path | dirname }}/gres.conf"
    mode: "0600"
    owner: slurm
    group: slurm
  when: openhpc_enable.control | default(false)
  notify:
    - Restart slurmctld service
  register: ohpc_gres_conf
  # NB uses restart rather than reload as this is needed in some cases

- name: Template cgroup.conf
  # appears to be required even with NO cgroup plugins: https://slurm.schedmd.com/cgroups.html#cgroup_design
  template:
    src: "{{ openhpc_cgroup_template }}"
    dest: "{{ openhpc_slurm_conf_path | dirname }}/cgroup.conf"
    mode: "0644" # perms/ownership based off src from ohpc package
    owner: root
    group: root
  when: openhpc_enable.control | default(false)
  notify:
    - Restart slurmctld service
  register: ohpc_cgroup_conf
  # NB uses restart rather than reload as this is needed in some cases

- name: Template mpi.conf
  template:
    src: "{{ openhpc_mpi_template }}"
    dest: "{{ openhpc_slurm_conf_path | dirname }}/mpi.conf"
    owner: root
    group: root
    mode: "0644"
  when:
    - openhpc_enable.control | default(false)
    - openhpc_mpi_config | length > 0
  notify:
    - Restart slurmctld service
  register: ohpc_mpi_conf

# Workaround for https://bugs.rockylinux.org/view.php?id=10165
- name: Fix permissions on /etc for Munge service
  ansible.builtin.file:
    mode: g-w
    path: /etc

- name: Ensure Munge service is running
  service:
    name: munge
    state: "{{ 'restarted' if _openhpc_munge_key_copy.changed else 'started' }}"
  when: openhpc_slurm_service_started | bool

- name: Check slurmdbd state
  command: systemctl is-active slurmdbd # noqa: command-instead-of-module
  changed_when: false
  failed_when: false # rc = 0 when active
  register: _openhpc_slurmdbd_state

- name: Ensure slurm database is upgraded if slurmdbd inactive
  import_tasks: upgrade.yml # need import for conditional support
  when:
    - "_openhpc_slurmdbd_state.stdout == 'inactive'"
    - openhpc_enable.database | default(false)

- name: Notify handler for slurmd restart
  debug:
    msg: "notifying handlers" # meta: noop doesn't support 'when'
  changed_when: true
  when:
    - openhpc_slurm_control_host in ansible_play_hosts
    - hostvars[openhpc_slurm_control_host].ohpc_slurm_conf.changed or
      hostvars[openhpc_slurm_control_host].ohpc_mpi_conf.changed or
      hostvars[openhpc_slurm_control_host].ohpc_cgroup_conf.changed or
      hostvars[openhpc_slurm_control_host].ohpc_gres_conf.changed # noqa no-handler
  notify:
    - Restart slurmd service

- name: Configure slurmd command line options
  vars:
    slurmd_options_configless: "--conf-server {{ openhpc_slurm_control_host_address | default(openhpc_slurm_control_host) }}"
  lineinfile:
    path: /etc/sysconfig/slurmd
    line: "SLURMD_OPTIONS='{{ slurmd_options_configless }}'"
    regexp: "^SLURMD_OPTIONS="
    create: yes
    owner: root
    group: root
    mode: '0644'
  when:
    - openhpc_enable.batch | default(false)
  notify:
    - Restart slurmd service
  # Reloading is sufficent, but using a single handler means no bounce. Realistically this won't regularly change on a running slurmd so restarting is ok.

# Munge state could be unchanged but the service is not running.
# Handle that here.
- name: Configure Munge service
  service:
    name: munge
    enabled: "{{ openhpc_slurm_service_enabled | bool }}"
    state: "{{ 'started' if openhpc_slurm_service_started | bool else 'stopped' }}"

- name: Flush handler
  meta: flush_handlers # as then subsequent "ensure" is a no-op if slurm services bounced

- name: Ensure slurmdbd state
  service:
    name: slurmdbd
    enabled: "{{ openhpc_slurm_service_enabled | bool }}"
    state: "{{ 'started' if openhpc_slurm_service_started | bool else 'stopped' }}"
  when: openhpc_enable.database | default(false) | bool

- name: Ensure slurmctld state
  service:
    name: slurmctld
    enabled: "{{ openhpc_slurm_service_enabled | bool }}"
    state: "{{ 'started' if openhpc_slurm_service_started | bool else 'stopped' }}"
  when: openhpc_enable.control | default(false) | bool

- name: Ensure slurmd state
  service:
    name: slurmd
    enabled: "{{ openhpc_slurm_service_enabled | bool }}"
    state: "{{ 'started' if openhpc_slurm_service_started | bool else 'stopped' }}"
  when: openhpc_enable.batch | default(false) | bool

- import_tasks: facts.yml
